import streamlit as st
from google import genai
from google.genai import types
import os
from dotenv import load_dotenv
from PIL import Image

# 1. è¨­å®š
load_dotenv()
st.set_page_config(page_title="åŒ»å­¦éƒ¨åˆæ ¼AI", page_icon="ğŸ©º")

st.title("ğŸ©º åŒ»å­¦éƒ¨å—é¨“å¯¾ç­– AIå®¶åº­æ•™å¸«")
st.caption("æ±å¤§ãƒ»é †å¤©å ‚ãƒ»æ…¶æ‡‰ãªã©ã®éå»å•PDFã‚„ã€å›³è¡¨ã®è§£èª¬ã‚‚å¯èƒ½ã§ã™")

# --- â˜…ã“ã“ãŒä¿®æ­£ã—ãŸã€Œã‚­ãƒ¼èª­ã¿è¾¼ã¿éƒ¨åˆ†ã€ã§ã™â˜… ---
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    # Streamlit Cloudã®Secretsã‹ã‚‰æ¢ã™
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except:
        pass

if not api_key:
    st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãªã‚‰.envã€ã‚¯ãƒ©ã‚¦ãƒ‰ãªã‚‰Secretsã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
else:
    client = genai.Client(api_key=api_key)
# ------------------------------------------------

# 2. å±¥æ­´ã®ä¿å­˜
if "history" not in st.session_state:
    st.session_state.history = []
    st.session_state.history.append({"role": "model", "text": "ã“ã‚“ã«ã¡ã¯ï¼PDFã®éå»å•ã‚„ã€ç”»åƒã®è§£èª¬ã‚‚ä»»ã›ã¦ãã ã•ã„ã€‚"})

# 3. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¬„
with st.sidebar:
    st.header("ğŸ“‚ è³‡æ–™ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader("å•é¡Œ(PDF/ç”»åƒ)ã‚’ã“ã“ã«ãƒ‰ãƒ©ãƒƒã‚°", type=["jpg", "png", "jpeg", "pdf"])
    
    user_content = None
    
    if uploaded_file:
        if uploaded_file.type == "application/pdf":
            st.success(f"ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {uploaded_file.name}")
            user_content = types.Part.from_bytes(
                data=uploaded_file.getvalue(),
                mime_type="application/pdf"
            )
        else:
            user_content = Image.open(uploaded_file)
            st.image(user_content, caption="èª­ã¿è¾¼ã‚“ã ç”»åƒ", use_container_width=True)

# 4. ä¼šè©±å±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.history:
    with st.chat_message(message["role"]):
        st.write(message["text"])

# 5. å…¥åŠ›ã¨å®Ÿè¡Œ
prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

if prompt:
    with st.chat_message("user"):
        st.write(prompt)
    st.session_state.history.append({"role": "user", "text": prompt})

    with st.chat_message("assistant"):
        with st.spinner("è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§è€ƒãˆä¸­..."):
            try:
                system_instruction = """
                ã‚ãªãŸã¯åŒ»å­¦éƒ¨å—é¨“ã®ãƒ—ãƒ­å®¶åº­æ•™å¸«ã§ã™ã€‚
                PDFã‚„ç”»åƒãŒæä¾›ã•ã‚ŒãŸå ´åˆã¯ã€ãã®å†…å®¹ã‚’è©³ç´°ã«åˆ†æã—ã¦è§£èª¬ã—ã¦ãã ã•ã„ã€‚
                æ•°å¼ã¯LaTeXå½¢å¼ã§ã¯ãªãã€èª­ã¿ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
                """

                contents = [prompt]
                if user_content:
                    contents.insert(0, user_content)

                if 'client' in locals():
                    response = client.models.generate_content(
                        model="gemini-flash-latest",
                        contents=contents,
                        config=types.GenerateContentConfig(
                            system_instruction=system_instruction,
                            temperature=0.7,
                        )
                    )
                    st.write(response.text)
                    st.session_state.history.append({"role": "model", "text": response.text})
            
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
